{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NOTE - the GitHub viewer renders the formatting of this document incorrectly. I recommend either looking at the HTML version or downloading this notebook and opening in the notebook viewer of your choice."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Concatenate all fastas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "TOTAL=$(ls ../structures_sequences | wc -l)\n",
    "COUNT=0\n",
    "\n",
    "for FILE in ../structures_sequences/*fasta ; do\n",
    "\n",
    "if [[ $(basename $FILE) != PART* ]] ; then\n",
    "    cat $FILE >> all_full.fasta\n",
    "fi\n",
    "\n",
    "COUNT=$(($COUNT+1))\n",
    "echo \"$COUNT / $TOTAL\"\n",
    "\n",
    "done"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MMseqs clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "mkdir -p seq_cluster\n",
    "mmseqs easy-cluster \\\n",
    "all_full.fasta \\\n",
    "seq_cluster/seq_clusters \\\n",
    "seq_cluster/tmp \\\n",
    "--max-seqs 50000 \\\n",
    "-c 0.7 \\\n",
    "--cov-mode 0 \\\n",
    "--min-seq-id 0.2 \\\n",
    "--cluster-mode 0 \\\n",
    "--threads 5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Foldseek clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "# Collect representative structures\n",
    "mkdir -p seq_cluster/rep_structures\n",
    "COUNT=0\n",
    "cut -f1 seq_cluster/seq_clusters_cluster.tsv  | sort -u | while read LINE ; do \n",
    "    BASE=$(basename $LINE)\n",
    "\n",
    "    if [[ ! -f seq_cluster/rep_structures/${BASE}.pdb ]] ; then \n",
    "        cp /wynton/group/gladstone/users/jnomburg/projects/viral_structure/structure_symlinks/${BASE}.pdb seq_cluster/rep_structures\n",
    "    fi\n",
    "\n",
    "    COUNT=$(($COUNT+1))\n",
    "    echo $COUNT\n",
    "\n",
    "done\n",
    "\n",
    "# Run foldseek\n",
    "$CODE/vpSAT/bin/foldseek.sh \\\n",
    "-i seq_cluster/rep_structures \\\n",
    "-o foldseek/foldseek_clusters.m8 \\\n",
    "-C foldseek/ignoreme.tsv \\\n",
    "-t 5 \\\n",
    "-v 0.7 \\\n",
    "-c\n",
    "\n",
    "# Filter on TMscore\n",
    "sat.py aln_filter \\\n",
    "-a foldseek/foldseek_clusters.m8 \\\n",
    "-o foldseek/foldseek_clusters_mode0cov0.7_TMscore0.4.filt.m8 \\\n",
    "-f \"query,target,fident,alnlen,qlen,tlen,mismatch,gapopen,qstart,qend,tstart,tend,evalue,bits,alntmscore\" \\\n",
    "-m 0.4 \\\n",
    "-M 1 \\\n",
    "-x alntmscore\n",
    "\n",
    "# Generate a cluster file\n",
    "ls seq_cluster/rep_structures > foldseek/all_inputs.txt\n",
    "\n",
    "sat.py aln_cluster \\\n",
    "-a foldseek/foldseek_clusters_mode0cov0.7_TMscore0.4.filt.m8 \\\n",
    "-o foldseek/foldseek_clusters.tsv \\\n",
    "-A foldseek/all_inputs.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Merge structure and sequence cluster files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "mkdir -p merged_clusters\n",
    "\n",
    "sat.py aln_expand_clusters \\\n",
    "-c foldseek/foldseek_clusters.tsv \\\n",
    "-s seq_cluster/seq_clusters_cluster.tsv \\\n",
    "-o merged_clusters/merged_clusters.tsv \\\n",
    "-F \"cluster_rep,cluster_member\" \\\n",
    "-f \"cluster_rep,cluster_member\"\n",
    "\n",
    "# Generate counts file. This wasn't really used.\n",
    "sat.py aln_taxa_counts \\\n",
    "-c merged_clusters/merged_clusters.tsv \\\n",
    "-o merged_clusters/merged_clusters.counts.tsv \\\n",
    "-F \"cluster_ID,cluster_rep,subcluster_rep,cluster_member,cluster_count\"\n",
    "\n",
    "# Add taxonomy\n",
    "# This is adapting aln_add_taxonomy, which is deisnged for alignments rather than \n",
    "# cluster files.\n",
    "sat.py aln_add_taxonomy \\\n",
    "-a merged_clusters/merged_clusters.tsv \\\n",
    "-o merged_clusters/merged_clusters.tax.tsv.TEMP \\\n",
    "-f \"cluster_ID,cluster_rep,query,target,cluster_count\"\n",
    "\n",
    "# Reformat the taxonomy columns to general the file clusters file\n",
    "awk 'BEGIN {FS=OFS=\"\\t\"}\n",
    "NR==1 {\n",
    "    for (i=1; i<=NF; i++) {\n",
    "        if ($i == \"query\") { \n",
    "            $i = \"subcluster_rep\"; \n",
    "            col[i]=1; \n",
    "        } else if ($i == \"target\") { \n",
    "            $i = \"cluster_member\"; \n",
    "            col[i]=1; \n",
    "        } else if ($i ~ /^target_/) { \n",
    "            $i = substr($i, 8); \n",
    "            col[i]=1; \n",
    "        } else if ($i ~ /^query_/) { \n",
    "            col[i]=0; \n",
    "        } else { \n",
    "            col[i]=1; \n",
    "        }\n",
    "    }\n",
    "}\n",
    "{\n",
    "    for (i=1; i<=NF; i++) {\n",
    "        if (col[i]) printf \"%s%s\", $i, (i<NF ? OFS : \"\\n\")\n",
    "    }\n",
    "}' merged_clusters/merged_clusters.tax.tsv.TEMP >  merged_clusters/merged_clusters.tax.tsv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create connection map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "# This is just for making the family-family network\n",
    "sat.py aln_connection_map \\\n",
    "-c merged_clusters/merged_clusters.tax.tsv \\\n",
    "-o merged_clusters/connection_map.tsv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Run DALI to compare reps from all 5.7K-ish protein clusters that have more than 1 member"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "# First collect the structures\n",
    "mkdir -p dali_euk_vs_euk/strucs\n",
    "COUNT=0\n",
    "awk '$5 > 1'  merged_clusters/merged_clusters.tsv | cut -f2 | sort -u | while read LINE ; do\n",
    "    cp seq_cluster/rep_structures/${LINE}.pdb dali_euk_vs_euk/strucs\n",
    "    COUNT=$(($COUNT+1))\n",
    "    echo \"$COUNT\"\n",
    "done\n",
    "\n",
    "# Import to DALI\n",
    "$CODE/vpSAT/bin/dali_format_inputs.sh \\\n",
    "-d dali_euk_vs_euk/strucs \\\n",
    "-o dali_euk_vs_euk/euk_dali_db \\\n",
    "-s dali_euk_vs_euk/euk_dali_key.tsv \\\n",
    "-b ~/phage_dali/phage_structure_key.txt \\\n",
    "-L dali_euk_vs_euk/euk_dali_symlinks\n",
    "\n",
    "# Prepare an SGE array\n",
    "$CODE/vpSAT/bin/prepare_job_array_sge.sh \\\n",
    "-d dali_euk_vs_euk/euk_dali_db \\\n",
    "-J dali_euk_vs_euk/dali_lists \\\n",
    "-N 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "# Running the array in an SGE submission\n",
    "LIST=$(sed \"${SGE_TASK_ID}q;d\" dali_euk_vs_euk/dali_lists_lists/sublist_list.txt)\n",
    "\n",
    "TEMP=${SGE_TASK_ID}__$RANDOM\n",
    "\n",
    "echo \"Copying over queries...\"\n",
    "cat $LIST | while read LINE ; do\n",
    "    FILE=dali_euk_vs_euk/euk_dali_db/$LINE\n",
    "    mkdir -p $TEMP\n",
    "    mkdir $TEMP/query\n",
    "    cp $FILE $TEMP/query\n",
    "done\n",
    "\n",
    "cd $TEMP\n",
    "\n",
    "# Make a copy of the full db here \n",
    "echo \"Copying over the target directory\"\n",
    "cp -r path/to/db target\n",
    "\n",
    "# Copy the query(s) to the target db so I can get qlen\n",
    "# NOTE - this isn't necessary for this particular search, bc it's already all-by-all\n",
    "echo \"Copying the query to the target dir too\"\n",
    "cp query/* target\n",
    "\n",
    "echo \"running the search\"\n",
    "$CODE/vpSAT/bin/dali.sh \\\n",
    "-q query \\\n",
    "-t target \\\n",
    "-o path_to/euk_dali_result \\\n",
    "-n 5\n",
    "\n",
    "cd ..\n",
    "\n",
    "rm -r $TEMP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "# Parsing the DALI results\n",
    "IN_DIR=path_to/euk_dali_result\n",
    "OUT_DIR=path_to/euk_dali_parsed\n",
    "\n",
    "for FILE in $IN_DIR/* ; do\n",
    "\n",
    "sat.py aln_parse_dali \\\n",
    "-a $FILE \\\n",
    "-o ${OUT_DIR}/$(basename ${FILE%.txt}).m8 \\\n",
    "-s dali_euk_vs_euk/euk_dali_key.tsv\n",
    "\n",
    "done"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "# Filter: Remove self alignments, filter for Z >= alnlen/10 -4, alnlen > 120\n",
    "awk -F '\\t' 'NR==1 || ($11 >= ($5/10) - 4)'  dali_euk_vs_euk.m8 | awk '$1 != $2' | awk '$5 >= 120' > dali_euk_vs_euk.filt.m8"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Running InterProScan on all sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "for FILE in pasth/to/structures_sequences/*fasta ; do\n",
    "\n",
    "    cat $FILE >> all.fasta\n",
    "\n",
    "done\n",
    "\n",
    "FASTA=all.fasta\n",
    "\n",
    "interproscan.sh \\\n",
    "-i $FASTA \\\n",
    "-f tsv \\\n",
    "-appl TIGRFAM,Pfam,CDD \\\n",
    "-o interproscan_PFAM_TIGRFAM_CDD.tsv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Running DALI to determine cluster purity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Copying and organizing the structures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "STRUCS=path/to/my/structures\n",
    "\n",
    "COUNT=0\n",
    "\n",
    "awk '$5 >= 100' merged_clusters.tax.tsv | awk '$1 != \"cluster_ID\"' | while read LINE ; do\n",
    "\n",
    "CLUSTER_ID=$(echo $LINE | awk '{print $1}')\n",
    "CLUSTER_REP=$(echo $LINE | awk '{print $2}')\n",
    "CLUSTER_MEMBER=$(echo $LINE | awk '{print $4}')\n",
    "\n",
    "mkdir -p clusters/cluster_${CLUSTER_ID}/{structures,rep_structure}\n",
    "\n",
    "# Copy the rep if necessary\n",
    "if [[ ! -f clusters/cluster_${CLUSTER_ID}/rep_structure/${CLUSTER_REP}.pdb ]] ; then\n",
    "    cp $STRUCS/${CLUSTER_REP}.pdb clusters/cluster_${CLUSTER_ID}/rep_structure/\n",
    "fi\n",
    "\n",
    "if [[ $CLUSTER_REP == $CLUSTER_MEMBER ]] ; then\n",
    "    continue\n",
    "fi\n",
    "\n",
    "# Copy the members\n",
    "if [[ ! -f clusters/cluster_${CLUSTER_ID}/structures/${CLUSTER_MEMBER}.pdb ]] ; then\n",
    "    cp $STRUCS/${CLUSTER_MEMBER}.pdb clusters/cluster_${CLUSTER_ID}/structures\n",
    "fi\n",
    "\n",
    "COUNT=$(($COUNT+1))\n",
    "echo $COUNT\n",
    "\n",
    "done"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "SGE Array to run the searches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "#!/bin/bash\n",
    "#$ -S /bin/bash\n",
    "#$ -o ./\n",
    "#$ -e ./\n",
    "#$ -cwd\n",
    "#$ -r y\n",
    "#$ -j y\n",
    "#$ -l mem_free=10G\n",
    "#$ -l scratch=10G\n",
    "#$ -l h_rt=2:00:00\n",
    "#$ -t 1-57\n",
    "\n",
    "CLUSTER_DIR=clusters/cluster_${SGE_TASK_ID}\n",
    "\n",
    "cd $CLUSTER_DIR\n",
    "\n",
    "# Make the databases\n",
    "echo \"Making the structures database\"\n",
    "$CODE/vpSAT/bin/dali_format_inputs.sh \\\n",
    "-d structures \\\n",
    "-o structure_db \\\n",
    "-s structures_key_NOREP.txt \\\n",
    "-L structures_symlink\n",
    "\n",
    "echo \"Making the rep database\"\n",
    "$CODE/vpSAT/bin/dali_format_inputs.sh \\\n",
    "-d rep_structure \\\n",
    "-o rep_structure_db \\\n",
    "-s structures_key_REP_ONLY.txt \\\n",
    "-L rep_structure_db_symlink \\\n",
    "-b structures_key_NOREP.txt\n",
    "\n",
    "# The final key\n",
    "echo \"Merging the key\"\n",
    "cat structures_key*txt > structures_key.txt\n",
    "\n",
    "# Run DALI\n",
    "echo \"Running DALI\"\n",
    "$CODE/vpSAT/bin/dali.sh \\\n",
    "-q rep_structure_db \\\n",
    "-t structure_db \\\n",
    "-o dali_result\n",
    "\n",
    "# Parse DALI\n",
    "echo \"Parsing the dali result\"\n",
    "conda activate SAT\n",
    "for FILE in dali_result/*txt ; do\n",
    "sat.py aln_parse_dali \\\n",
    "-a $FILE \\\n",
    "-s structures_key.txt \\\n",
    "-o cluster_${SGE_TASK_ID}_result.m8\n",
    "done"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "# Collect results\n",
    "cat clusters/*/*m8 > dali_cluster_purity.m8"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Aligning Virus protein cluster representatives against the 2.3M AFDB cluster representatives"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Downloading the reps"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Python script for the download"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import json\n",
    "import os\n",
    "import sys\n",
    "from urllib.request import urlopen, HTTPError\n",
    "from concurrent.futures import ThreadPoolExecutor\n",
    "\n",
    "def download_af_pdb(accession, outdir):\n",
    "    url = f\"https://alphafold.ebi.ac.uk/api/prediction/{accession}\"\n",
    "    try:\n",
    "        with urlopen(url) as res:\n",
    "            payload = res.read().decode(\"utf-8\")\n",
    "            obj = json.loads(payload)\n",
    "            pdb_url = obj[0][\"pdbUrl\"]\n",
    "    except HTTPError as e:\n",
    "        if e.code == 404:\n",
    "            sys.stderr.write(f\"Accession {accession} not found, skipping...\\n\")\n",
    "            return\n",
    "        else:\n",
    "            raise  # Re-raise the exception for other HTTP errors\n",
    "        \n",
    "    filename = os.path.basename(pdb_url)\n",
    "    filepath = os.path.join(outdir, filename)\n",
    "\n",
    "    # Only download if the file does not exist\n",
    "    if not os.path.exists(filepath):\n",
    "        with open(filepath, \"wb\") as fh, urlopen(pdb_url) as res:\n",
    "            for chunk in res:\n",
    "                fh.write(chunk)\n",
    "\n",
    "def main():\n",
    "    if len(sys.argv) != 3:\n",
    "        sys.stderr.write(\"Usage: python download_af_pdb_from_tsv.py input.tsv outdir\\n\")\n",
    "        sys.exit(1)\n",
    "\n",
    "    input_tsv = sys.argv[1]\n",
    "    outdir = sys.argv[2]\n",
    "\n",
    "    # Ensure output directory exists\n",
    "    if not os.path.exists(outdir):\n",
    "        os.makedirs(outdir)\n",
    "\n",
    "    # Read existing filenames in the output directory\n",
    "    existing_files = set(os.listdir(outdir))\n",
    "\n",
    "    with ThreadPoolExecutor() as executor, open(input_tsv, 'r') as tsv_file:\n",
    "        reader = csv.reader(tsv_file, delimiter='\\t')\n",
    "        next(reader)  # Skip the header row if present\n",
    "\n",
    "        # Filter out accessions that have already been downloaded\n",
    "        tasks = []\n",
    "        for row in reader:\n",
    "            accession = row[0]\n",
    "            filename = f\"{accession}.pdb\"  # Assuming the files are saved as '<accession>.pdb'\n",
    "            if filename not in existing_files:\n",
    "                task = executor.submit(download_af_pdb, accession, outdir)\n",
    "                tasks.append(task)\n",
    "\n",
    "        # Wait for all futures to complete\n",
    "        for future in tasks:\n",
    "            future.result()\n",
    "            \n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Download using 2-repId_isDark_nMem_repLen_avgLen_repPlddt_avgPlddt_LCAtaxId.tsv.gz from https://afdb-cluster.steineggerlab.workers.dev/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "python3 download_reps.py 2-repId_isDark_nMem_repLen_avgLen_repPlddt_avgPlddt_LCAtaxId.tsv reps"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Running the search"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Make foldseek database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "#!/bin/bash\n",
    "#$ -S /bin/bash\n",
    "#$ -o ./\n",
    "#$ -e ./\n",
    "#$ -cwd\n",
    "#$ -r y\n",
    "#$ -j y\n",
    "#$ -l mem_free=7G\n",
    "#$ -l scratch=10G\n",
    "#$ -l h_rt=40:00:00\n",
    "#$ -pe smp 8\n",
    "\n",
    "conda activate vpSAT\n",
    "\n",
    "STRUCS=reps\n",
    "DB_DIR=db\n",
    "\n",
    "foldseek createdb $STRUCS ${DB_DIR}/AF2db_reps_db --threads 8"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Doing the alignment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "#!/bin/bash\n",
    "#$ -S /bin/bash\n",
    "#$ -o ./\n",
    "#$ -e ./\n",
    "#$ -cwd\n",
    "#$ -r y\n",
    "#$ -j y\n",
    "#$ -l mem_free=7G\n",
    "#$ -l scratch=10G\n",
    "#$ -l h_rt=40:00:00\n",
    "#$ -pe smp 8\n",
    "\n",
    "conda activate vpSAT\n",
    "\n",
    "STRUCS=/path/to/virus/cluster_reps/strucs\n",
    "DB=path/to/db/AF2db_reps_db\n",
    "\n",
    "$CODE/vpSAT/bin/foldseek.sh \\\n",
    "-i $STRUCS \\\n",
    "-o vir_protein_reps_vs_AF2_reps.m8 \\\n",
    "-d $DB \\\n",
    "-t 8 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Filter on TMscore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "# TMscore filtering\n",
    "sat.py aln_filter \\\n",
    "-a vir_protein_reps_vs_AF2_reps.m8 \\\n",
    "-o vir_protein_reps_vs_AF2_reps.TMscorefilt.m8 \\\n",
    "-f 'query,target,fident,alnlen,qlen,tlen,mismatch,gapopen,qstart,qend,tstart,tend,evalue,bits,alntmscore'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Benchmarking structure vs sequence searches"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Benchmarking virus-virus comparisons"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prepare proteins"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Steps:\n",
    "1. In R, find protein clusters that have more than one sequence cluster  \n",
    "2. Use seqtk subseq to generate one fasta per protein cluster. \n",
    "-- Here, first use a python script to write a single text file with all member names for each cluster\n",
    "--- then, use each of those files as input to seqtk subseq\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is make_cluster_lists.py\n",
    "\n",
    "import pandas as pd\n",
    "import os\n",
    "import sys\n",
    "\n",
    "# Check for correct usage\n",
    "if len(sys.argv) < 2:\n",
    "    print(\"Usage: python script.py <input_file>\")\n",
    "    sys.exit(1)\n",
    "\n",
    "# The first command line argument is the script name, so the second one is the input file\n",
    "input_file_path = sys.argv[1]\n",
    "\n",
    "# Ensure the directory for the cluster files exists\n",
    "output_dir = \"cluster_lists\"\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "# Load the dataset, assuming tab separation\n",
    "df = pd.read_csv(input_file_path, sep='\\t')\n",
    "\n",
    "# Get the unique cluster IDs\n",
    "unique_cluster_ids = df['cluster_ID'].unique()\n",
    "\n",
    "# Iterate over each unique cluster ID and create a separate file\n",
    "for cluster_id in unique_cluster_ids:\n",
    "    cluster_df = df[df['cluster_ID'] == cluster_id]\n",
    "    output_file_path = os.path.join(output_dir, f\"{cluster_id}.tsv\")\n",
    "    \n",
    "    # Write to file, including the header\n",
    "    cluster_df.to_csv(output_file_path, sep='\\t', index=False, header=False)\n",
    "\n",
    "print(f\"Finished creating individual files for each cluster ID in {output_dir}.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "# R code\n",
    "# Finding protein clusters with more than 1 sequence representative\n",
    "\n",
    "multi_seq_rep_list <- clusters %>%\n",
    "  select(cluster_ID, seq_rep) %>%\n",
    "  distinct() %>%\n",
    "  group_by(cluster_ID) %>%\n",
    "  summarize(n = n_distinct(seq_rep)) %>%\n",
    "  filter(n > 1) %>%\n",
    "  pull(cluster_ID)\n",
    "\n",
    "clusters %>%\n",
    "  filter(cluster_ID %in% multi_seq_rep_list) %>%\n",
    "  write_tsv(\"outputs/clusters_multi_seq_reps_only.tsv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "# Generate input lists for seqtk subseq\n",
    "python make_cluster_lists.py  clusters_multi_seq_reps_only.tsv \n",
    "\n",
    "for FILE in cluster_lists/*tsv ; do \n",
    "\n",
    "cut -f 4 $FILE > ${FILE}.list\n",
    "\n",
    "done"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "FASTA=all_full.fasta\n",
    "\n",
    "mkdir -p cluster_fastas\n",
    "\n",
    "for FILE in cluster_lists/*list ; do\n",
    "\n",
    "seqtk subseq $FASTA $FILE > cluster_fastas/$(basename ${FILE%.tsv.list}).fasta\n",
    "\n",
    "done"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DIAMOND"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "# Prepare an SGE job array\n",
    "$CODE/vpSAT/bin/prepare_job_array_sge.sh \\\n",
    "-d cluster_fastas \\\n",
    "-N 10 \\\n",
    "-J arrays"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "# Run DIAMOND\n",
    "\n",
    "\n",
    "#!/bin/bash\n",
    "#$ -S /bin/bash\n",
    "#$ -o ./\n",
    "#$ -e ./\n",
    "#$ -cwd\n",
    "#$ -r y\n",
    "#$ -j y\n",
    "#$ -l mem_free=10G\n",
    "#$ -l scratch=10G\n",
    "#$ -l h_rt=1:00:00\n",
    "#$ -t 1-86\n",
    "\n",
    "LIST=$(sed \"${SGE_TASK_ID}q;d\" arrays_lists/sublist_list.txt)\n",
    "\n",
    "mkdir -p diamond/alignment_files\n",
    "mkdir -p diamond/cluster_files\n",
    "\n",
    "cat $LIST | while read LINE ; do\n",
    "\n",
    "FILE=cluster_fastas/$LINE\n",
    "\n",
    "DB=${FILE%.fasta}\n",
    "BASE=$(basename ${FILE%.fasta})\n",
    "\n",
    "conda activate vpSAT\n",
    "\n",
    "diamond makedb --in $FILE -d $DB\n",
    "\n",
    "diamond blastp \\\n",
    "-d $DB \\\n",
    "-q $FILE \\\n",
    "-o diamond/alignment_files/${BASE}.tsv \\\n",
    "--outfmt 6 \\\n",
    "--threads 5 \\\n",
    "--id 0.2 \\\n",
    "--query-cover 0.7 \\\n",
    "--subject-cover 0.7 \\\n",
    "--more-sensitive\n",
    "\n",
    "conda deactivate\n",
    "\n",
    "conda activate SAT\n",
    "sat.py aln_cluster \\\n",
    "-a diamond/alignment_files/${BASE}.tsv \\\n",
    "-o diamond/cluster_files/${BASE}.cluster.tsv \\\n",
    "-f \"query,target,pident,length,mismatch,gapopen,qstart,qend,sstart,send,evalue,bitscore\" \\\n",
    "-A cluster_lists/${BASE}.tsv.list\n",
    "\n",
    "# also count total number of unique clusters\n",
    "CLUSTER_COUNT=$(cut -f1 diamond/cluster_files/${BASE}.cluster.tsv | sort -u | wc -l)\n",
    "N_IN_LARGEST_CLUSTER=$(cut -f1 diamond/cluster_files/${BASE}.cluster.tsv | uniq -c | sort -nr | head -n1 | awk '{print $1}')\n",
    "TOTAL=$(wc -l diamond/cluster_files/${BASE}.cluster.tsv | awk '{print $1}')\n",
    "echo -e \"${BASE}\\t${CLUSTER_COUNT}\\t${N_IN_LARGEST_CLUSTER}\\t${TOTAL}\" >> diamond/diamond_cluster_counts.tsv\n",
    "\n",
    "done\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MMseqs2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "#!/bin/bash\n",
    "#$ -S /bin/bash\n",
    "#$ -o ./\n",
    "#$ -e ./\n",
    "#$ -cwd\n",
    "#$ -r y\n",
    "#$ -j y\n",
    "#$ -l mem_free=10G\n",
    "#$ -l scratch=10G\n",
    "#$ -l h_rt=1:00:00\n",
    "#$ -t 1-86\n",
    "\n",
    "LIST=$(sed \"${SGE_TASK_ID}q;d\" arrays_lists/sublist_list.txt)\n",
    "\n",
    "mkdir -p mmseqs2/alignment_files\n",
    "mkdir -p mmseqs2/cluster_files\n",
    "\n",
    "cat $LIST | while read LINE ; do\n",
    "\n",
    "FILE=cluster_fastas/$LINE\n",
    "\n",
    "conda activate vpSAT\n",
    "\n",
    "RAND=${RANDOM}_${RANDOM}\n",
    "BASE=$(basename ${FILE%.fasta})\n",
    "\n",
    "mmseqs easy-search \\\n",
    "$FILE \\\n",
    "$FILE \\\n",
    "mmseqs2/alignment_files/${BASE}.m8 \\\n",
    "$RAND \\\n",
    "--max-seqs 50000 \\\n",
    "-c 0.7 \\\n",
    "--cov-mode 0 \\\n",
    "--min-seq-id 0.2 \\\n",
    "--threads 5\n",
    "\n",
    "rm -r $RAND\n",
    "\n",
    "conda activate SAT\n",
    "sat.py aln_cluster \\\n",
    "-a mmseqs2/alignment_files/${BASE}.m8 \\\n",
    "-o mmseqs2/cluster_files/${BASE}.cluster.tsv \\\n",
    "-f \"query,target,fident,alnlen,mismatch,gapopen,qstart,qend,tstart,tend,evalue,bits\" \\\n",
    "-A cluster_lists/${BASE}.tsvncd ...list\n",
    "\n",
    "# also count total number of unique clusters\n",
    "CLUSTER_COUNT=$(cut -f1 mmseqs2/cluster_files/${BASE}.cluster.tsv | sort -u | wc -l)\n",
    "N_IN_LARGEST_CLUSTER=$(cut -f1 mmseqs2/cluster_files/${BASE}.cluster.tsv | uniq -c | sort -nr | head -n1 | awk '{print $1}')\n",
    "TOTAL=$(wc -l mmseqs2/cluster_files/${BASE}.cluster.tsv | awk '{print $1}')\n",
    "echo -e \"${BASE}\\t${CLUSTER_COUNT}\\t${N_IN_LARGEST_CLUSTER}\\t${TOTAL}\" >> mmseqs2/mmseqs_cluster_counts.tsv\n",
    "\n",
    "done"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### jackhmmer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "#!/bin/bash\n",
    "#$ -S /bin/bash\n",
    "#$ -o ./\n",
    "#$ -e ./\n",
    "#$ -cwd\n",
    "#$ -r y\n",
    "#$ -j y\n",
    "#$ -l mem_free=10G\n",
    "#$ -l scratch=10G\n",
    "#$ -l h_rt=4:00:00\n",
    "#$ -t 1-86\n",
    "\n",
    "LIST=$(sed \"${SGE_TASK_ID}q;d\" arrays_lists/sublist_list.txt)\n",
    "\n",
    "mkdir -p jackhmmer/alignment_files\n",
    "mkdir -p jackhmmer/cluster_files\n",
    "\n",
    "cat $LIST | while read LINE ; do\n",
    "\n",
    "FILE=cluster_fastas/$LINE\n",
    "\n",
    "DB=${FILE%.fasta}\n",
    "BASE=$(basename ${FILE%.fasta})\n",
    "\n",
    "conda activate hmmer\n",
    "\n",
    "jackhmmer \\\n",
    "--tblout jackhmmer/alignment_files/${BASE}.jackhmmer.txt.tmp \\\n",
    "-E 0.01 \\\n",
    "--cpu 5 \\\n",
    "-N 3 \\\n",
    "$FILE \\\n",
    "$FILE \n",
    "\n",
    "# Parse the jackhmmer output into a better file...\n",
    "# colnames are target,query,evalue,bits\n",
    "awk 'NR > 3 {print $1, $3, $5, $6}' FS=\" \" OFS=\"\\t\" jackhmmer/alignment_files/${BASE}.jackhmmer.txt.tmp > jackhmmer/alignment_files/${BASE}.jackhmmer.txt.tmp2\n",
    "\n",
    "# There are a few lines starting with #, need to remove them\n",
    "grep -v \"^#\" jackhmmer/alignment_files/${BASE}.jackhmmer.txt.tmp2 > jackhmmer/alignment_files/${BASE}.jackhmmer.txt\n",
    "\n",
    "# Make cluster file\n",
    "conda deactivate\n",
    "conda activate SAT\n",
    "\n",
    "sat.py aln_cluster \\\n",
    "-a jackhmmer/alignment_files/${BASE}.jackhmmer.txt \\\n",
    "-o jackhmmer/cluster_files/${BASE}.jackhmmer.cluster.tsv \\\n",
    "-f \"target,query,evalue,bits\" \\\n",
    "-A cluster_lists/${BASE}.tsv.list\n",
    "\n",
    "# also count total number of unique clusters\n",
    "CLUSTER_COUNT=$(cut -f1 jackhmmer/cluster_files/${BASE}.jackhmmer.cluster.tsv | sort -u | wc -l)\n",
    "N_IN_LARGEST_CLUSTER=$(cut -f1 jackhmmer/cluster_files/${BASE}.jackhmmer.cluster.tsv | uniq -c | sort -nr | head -n1 | awk '{print $1}')\n",
    "TOTAL=$(wc -l jackhmmer/cluster_files/${BASE}.jackhmmer.cluster.tsv | awk '{print $1}')\n",
    "echo -e \"${BASE}\\t${CLUSTER_COUNT}\\t${N_IN_LARGEST_CLUSTER}\\t${TOTAL}\" >> jackhmmer/jackhmmer_cluster_counts.tsv\n",
    "\n",
    "conda deactivate\n",
    "\n",
    "done"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Searches for Extended data fig 8D\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    " # MMseqs2\n",
    "QUERY=queries.fasta\n",
    "TARGET=/wynton/group/gladstone/users/jnomburg/projects/viral_structure/2023-09-07_whole_protein_clustering_full/all_full.fasta\n",
    "\n",
    "mkdir -p mmseqs2_tmp\n",
    "\n",
    "mmseqs easy-search \\\n",
    "$QUERY \\\n",
    "$TARGET \\\n",
    "mmseqs2.m8 \\\n",
    "mmseqs2_tmp \\\n",
    "--max-seqs 50000 \\\n",
    "--threads 5\n",
    "\n",
    "# DIAMOND\n",
    "QUERY=queries.fasta\n",
    "TARGET=/wynton/group/gladstone/users/jnomburg/projects/viral_structure/2023-09-07_whole_protein_clustering_full/all_full.fasta\n",
    "\n",
    "\n",
    "diamond makedb --in $TARGET -d all_full\n",
    "\n",
    "diamond blastp \\\n",
    "-d all_full.dmnd \\\n",
    "-q $QUERY \\\n",
    "-o diamond.m8 \\\n",
    "--outfmt 6 \\\n",
    "--threads 5 \\\n",
    "--more-sensitive\n",
    "\n",
    "# Jackhmmer\n",
    "QUERY=queries.fasta\n",
    "TARGET=/wynton/group/gladstone/users/jnomburg/projects/viral_structure/2023-09-07_whole_protein_clustering_full/all_full.fasta\n",
    "\n",
    "jackhmmer \\\n",
    "--tblout jackhmmer.m8.tmp1 \\\n",
    "--cpu 5 \\\n",
    "-N 3 \\\n",
    "$QUERY \\\n",
    "$TARGET \n",
    "\n",
    "# Parse the jackhmmer output into a better file...\n",
    "# colnames are query,target,evalue,bits\n",
    "awk 'NR > 3 {print $3, $1, $5, $6}' FS=\" \" OFS=\"\\t\" jackhmmer.m8.tmp1 > jackhmmer.m8.tmp2 && rm jackhmmer.m8.tmp1\n",
    "\n",
    "# There are a few lines starting with #, need to remove them\n",
    "grep -v \"#\" jackhmmer.m8.tmp2 > jackhmmer.m8 && rm jackhmmer.m8.tmp2\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## HHpred vs DALI benchmark"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Establishing benchmark set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "# see fig2.qmd, it's towards the end. This code requires some existing objects"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Running HHpred (aka HHblits --> HHsearch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "# Prepare a job array\n",
    "\n",
    "$CODE/vpSAT/bin/prepare_job_array_sge.sh \\\n",
    "-d /wynton/group/gladstone/users/jnomburg/projects/viral_structure_newer_directories/viral_structure_03/2024-05-02_preparing_benchmark_set/fastas \\\n",
    "-N 150 \\\n",
    "-j array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "#!/bin/bash\n",
    "#$ -S /bin/bash\n",
    "#$ -o ./\n",
    "#$ -e ./\n",
    "#$ -cwd\n",
    "#$ -r y\n",
    "#$ -j y\n",
    "#$ -l mem_free=20G\n",
    "#$ -l scratch=10G\n",
    "#$ -l h_rt=12:00:00\n",
    "#$ -l ssd_scratch=1\n",
    "#$ -l scratch=200G\n",
    "#$ -pe smp 10\n",
    "#$ -t 1-9\n",
    "\n",
    "conda activate hhsuite\n",
    "\n",
    "LIST=$(sed \"${SGE_TASK_ID}q;d\" _lists/sublist_list.txt)\n",
    "\n",
    "DATE=$(date)\n",
    "echo \"STARTING at $DATE\"\n",
    "\n",
    "UNIREF=/path/UniRef30_2023_02\n",
    "PDB=/path/pdb70\n",
    "\n",
    "mkdir -p hhblits_uniref_results\n",
    "mkdir -p hhsearch_pdb_results\n",
    "\n",
    "TEMP=${RANDOM}_${RANDOM}\n",
    "echo \"TEMP is $TEMP\"\n",
    "mkdir -p $TEMP/query_fastas\n",
    "\n",
    "# Copy the fastas over\n",
    "cat $LIST | while read LINE ; do\n",
    "FILE=/wynton/group/gladstone/users/jnomburg/projects/viral_structure_newer_directories/viral_structure_03/2024-05-02_preparing_benchmark_set/fastas/$LINE\n",
    "cp $FILE $TEMP/query_fastas\n",
    "done\n",
    "\n",
    "echo \"Copying UNIREF to TMPDIR\"\n",
    "time cp ${UNIREF}* $TMPDIR\n",
    "\n",
    "echo \"Running hhblits\"\n",
    "for FILE in $TEMP/query_fastas/*fasta ; do\n",
    "\n",
    "BASE=$(basename ${FILE%.fasta})\n",
    "\n",
    "hhblits \\\n",
    "-i $FILE \\\n",
    "-d $TMPDIR/UniRef30_2023_02 \\\n",
    "-blasttab hhblits_uniref_results/${BASE}.uniref.m8 \\\n",
    "-oa3m hhblits_uniref_results/${BASE}.uniref.a3m \\\n",
    "-n 1 \\\n",
    "-cpu 10 \\\n",
    "-cov 20 \n",
    "done\n",
    "\n",
    "echo \"Removing uniref from TMPDIR\"\n",
    "rm $TMPDIR/UniRef30_2023_02*\n",
    "\n",
    "echo \"Copying PDB to TMPDIR\"\n",
    "time cp ${PDB}* $TMPDIR\n",
    "\n",
    "echo \"Running hhsearch\"\n",
    "for FILE in hhblits_uniref_results/*a3m ; do\n",
    "\n",
    "BASE=$(basename ${FILE%.a3m})\n",
    "\n",
    "time hhsearch \\\n",
    "-i $FILE \\\n",
    "-d $TMPDIR/pdb70 \\\n",
    "-blasttab hhsearch_pdb_results/${BASE}.pdb.m8 \\\n",
    "-cov 20 \\\n",
    "-cpu 10\n",
    "done\n",
    "\n",
    "rm -r $TEMP\n",
    "\n",
    "DATE=$(date)\n",
    "echo \"ENDED at $DATE\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "# Filter for E < 0.001\n",
    "COUNT=0\n",
    "\n",
    "for FILE in hhsearch_pdb_results/* ; do\n",
    "\n",
    "awk '$11 < 0.001' $FILE  >> 2024-05-02_benchmark_hhsearch_E0.001.m8\n",
    "\n",
    "COUNT=$(($COUNT+1))\n",
    "echo $COUNT\n",
    "\n",
    "done"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Running DALI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "$CODE/vpSAT/bin/prepare_job_array_slurm.sh \\\n",
    "-d benchmark_viral_dali_db \\\n",
    "-N 10 \\\n",
    "-J array10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "#!/bin/bash\n",
    "#SBATCH --account=ac_ribosome\n",
    "#SBATCH --partition=lr6\n",
    "#SBATCH --qos=lr_normal\n",
    "#SBATCH --ntasks=20\n",
    "#SBATCH --cpus-per-task=1\n",
    "#SBATCH --time=20:00:00\n",
    "#SBATCH --array=1-132\n",
    "\n",
    "module load gcc openmpi\n",
    "\n",
    "PDB=/path/pdb25_db\n",
    "\n",
    "LIST=$(sed \"${SLURM_ARRAY_TASK_ID}q;d\" array10_lists/sublist_list.txt)\n",
    "\n",
    "echo \"Starting. SLURM_ARRAY_TASK_ID is $SLURM_ARRAY_TASK_ID\"\n",
    "\n",
    "mkdir -p dali_results\n",
    "\n",
    "# Set up temp dir\n",
    "TEMP=${RANDOM}_${RANDOM}\n",
    "mkdir -p ${TEMP}/query_db\n",
    "echo \"TEMP: $TEMP\"\n",
    "\n",
    "# Copy over query\n",
    "cat $LIST | while read LINE ; do\n",
    "FILE=benchmark_viral_dali_db/$LINE\n",
    "cp $FILE ${TEMP}/query_db\n",
    "done\n",
    "\n",
    "# Copy over target\n",
    "cp -r $PDB ${TEMP}\n",
    "\n",
    "# Run DALI\n",
    "$CODE/vpSAT/bin/dali.sh \\\n",
    "-q ${TEMP}/query_db \\\n",
    "-t ${TEMP}/pdb25_db \\\n",
    "-o dali_results \\\n",
    "-n 20\n",
    "\n",
    "rm -r $TEMP\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "$CODE/vpSAT/bin/prepare_job_array_slurm.sh \\\n",
    "-d dali_results \\\n",
    "-N 10 \\\n",
    "-J parse_dali"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "#!/bin/bash\n",
    "#SBATCH --account=ac_ribosome\n",
    "#SBATCH --partition=lr6\n",
    "#SBATCH --qos=lr_normal\n",
    "#SBATCH --nodes=1\n",
    "#SBATCH --ntasks-per-node=1\n",
    "#SBATCH --cpus-per-task=1\n",
    "#SBATCH --time=0:50:00\n",
    "#SBATCH --array=1-132\n",
    "\n",
    "conda activate SAT\n",
    "\n",
    "mkdir -p dali_results_parsed\n",
    "\n",
    "LIST=$(sed \"${SLURM_ARRAY_TASK_ID}q;d\" parse_dali_lists/sublist_list.txt)\n",
    "\n",
    "cat $LIST | while read LINE ; do\n",
    "\n",
    "FILE=dali_results/$LINE\n",
    "BASE=$(basename ${FILE%.txt})\n",
    "\n",
    "sat.py aln_parse_dali \\\n",
    "-a $FILE \\\n",
    "-s benchmark_viral_key.txt \\\n",
    "-o dali_results_parsed/${BASE}.m8\n",
    "\n",
    "done"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "# Concatenate everything\n",
    "head -n1 dali_results_parsed/AEctA.m8 > 2024-05-02_benchmark_dali_results.m8\n",
    "for FILE in dali_results_parsed/* ; do\n",
    "tail -n +2 $FILE >> 2024-05-02_benchmark_dali_results.m8\n",
    "done"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Searching the transporter classification database"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Downloading TCDB classifications\n",
    "List of PDB structures per class: https://www.tcdb.org/cgi-bin/projectv/public/pdb.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "# R code\n",
    "tcdb <- read_tsv(\"inputs/tcdb_analysis/pdb.txt\", col_names = c(\"PDB\", \"TCDB_ID\", \"Classification\"))\n",
    "\n",
    "\n",
    "tcdb %>%\n",
    "  distinct() %>%\n",
    "  \n",
    "  # Weird, wonky, truncated\n",
    "  filter(PDB != \"1HXI\") %>%\n",
    "  \n",
    "  group_by(Classification) %>%\n",
    "  sample_n(size=5, replace=T) %>%\n",
    "  write_tsv(\"outputs/tcdb_5_per_type.tsv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Downloading PDBs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "COUNT=0\n",
    "tail -n +2 ../tcdb_5_per_type.tsv  | cut -f1 | while read LINE; do\n",
    "\n",
    "if [[ ! -f ${LINE}.pdb ]] ; then\n",
    "    wget https://files.rcsb.org/download/${LINE}.pdb\n",
    "fi\n",
    "\n",
    "COUNT=$(($COUNT+1))\n",
    "echo $COUNT\n",
    "\n",
    "done"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Making DALI databases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "mkdir -p tcdb_db\n",
    "for FILE in tcdb_pdbs/*pdb ; do\n",
    "\n",
    "PDB=$(basename ${FILE%.pdb})\n",
    "\n",
    "if [[ ! -f tcdb_db/${PDB}*.dat ]] ; then\n",
    "\n",
    "import.pl \\\n",
    "--pdbfile $FILE \\\n",
    "--pdbid $PDB \\\n",
    "--dat tcdb_db \\\n",
    "--clean\n",
    "\n",
    "fi\n",
    "\n",
    "done\n",
    "\n",
    "\n",
    "# Take just the first chain for multi-chain entries.\n",
    "mkdir tcdb_db_final\n",
    "\n",
    "for FILE in tcdb_pdbs/*pdb ; do\n",
    "\n",
    "PDB=$(basename ${FILE%.pdb})\n",
    "\n",
    "LIST=$(ls tcdb_db/${PDB}*)\n",
    "FIRST_FILE=$(echo $LIST | cut -d ' ' -f1)\n",
    "\n",
    "cp $FIRST_FILE tcdb_db_final\n",
    "\n",
    "done\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Spike in AFDB ENT1-4\n",
    "# note that the AFDB ENT1 and ENT2 were removed during downstream visualization/analysis in R.\n",
    "cd AFDB_ENT_PDBs\n",
    "\n",
    "wget https://alphafold.ebi.ac.uk/files/AF-Q99808-F1-model_v4.pdb && mv AF-Q99808-F1-model_v4.pdb ENT1.pdb\n",
    "wget https://alphafold.ebi.ac.uk/files/AF-Q14542-F1-model_v4.pdb && mv AF-Q14542-F1-model_v4.pdb ENT2.pdb\n",
    "wget https://alphafold.ebi.ac.uk/files/AF-Q9BZD2-F1-model_v4.pdb && mv AF-Q9BZD2-F1-model_v4.pdb ENT3.pdb\n",
    "wget https://alphafold.ebi.ac.uk/files/AF-Q7RTT9-F1-model_v4.pdb && mv AF-Q7RTT9-F1-model_v4.pdb ENT4.pdb\n",
    "\n",
    "for FILE in AFDB_ENT_PDBs/* ; do\n",
    "\n",
    "BASE=$(basename ${FILE%.pdb})\n",
    "\n",
    "import.pl \\\n",
    "--pdbfile $FILE \\\n",
    "--pdbid ${BASE} \\\n",
    "--dat tcdb_db_final \\\n",
    "--clean || echo \"${BASE} FAILED\"\n",
    "\n",
    "done"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "$CODE/vpSAT/bin/dali.sh \\\n",
    "-q query_db \\\n",
    "-t tcdb_db_tmp \\\n",
    "-o dali_result \\\n",
    "-d /wynton/group/gladstone/users/jnomburg/software/DaliLite.v5.newinstall/DaliLite.v5/bin/dali.pl\n",
    "\n",
    "\n",
    "conda activate SAT\n",
    "\n",
    "for FILE in dali_result/* ; do\n",
    "\n",
    "BASE=$(basename ${FILE%A.txt})\n",
    "\n",
    "sat.py aln_parse_dali \\\n",
    "-a $FILE \\\n",
    "-s fake_key.txt \\\n",
    "-o dali_result_parsed/${BASE}.m8\n",
    "\n",
    "done"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
